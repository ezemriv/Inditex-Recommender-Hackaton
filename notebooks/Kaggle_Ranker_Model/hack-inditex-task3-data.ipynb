{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb271060",
   "metadata": {
    "papermill": {
     "duration": 0.005383,
     "end_time": "2025-01-25T21:20:40.221172",
     "exception": false,
     "start_time": "2025-01-25T21:20:40.215789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6368a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T21:20:40.232724Z",
     "iopub.status.busy": "2025-01-25T21:20:40.232380Z",
     "iopub.status.idle": "2025-01-25T21:20:42.925467Z",
     "shell.execute_reply": "2025-01-25T21:20:42.924366Z"
    },
    "papermill": {
     "duration": 2.701587,
     "end_time": "2025-01-25T21:20:42.927630",
     "exception": false,
     "start_time": "2025-01-25T21:20:40.226043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from polars import StringCache\n",
    "\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd42fd",
   "metadata": {
    "papermill": {
     "duration": 0.005712,
     "end_time": "2025-01-25T21:20:42.939282",
     "exception": false,
     "start_time": "2025-01-25T21:20:42.933570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85906005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T21:20:42.950327Z",
     "iopub.status.busy": "2025-01-25T21:20:42.949758Z",
     "iopub.status.idle": "2025-01-25T21:20:42.955962Z",
     "shell.execute_reply": "2025-01-25T21:20:42.954685Z"
    },
    "papermill": {
     "duration": 0.013766,
     "end_time": "2025-01-25T21:20:42.957745",
     "exception": false,
     "start_time": "2025-01-25T21:20:42.943979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Processed Data Files\n",
    "PROCESSED_PATH = '/kaggle/input/inditex-users-recommender-hackatonnuwe-processed'\n",
    "USERS_DATA_PATH = os.path.join(PROCESSED_PATH, 'users_combined.parquet')\n",
    "TRAIN_PARQUET_PATH = os.path.join(PROCESSED_PATH, 'train.parquet')\n",
    "TEST_PARQUET_PATH = os.path.join(PROCESSED_PATH, 'test.parquet')\n",
    "PRODUCTS_PARQUET_PATH_IMPUTED = os.path.join(PROCESSED_PATH, 'products_imputed.parquet')\n",
    "\n",
    "# Engineered Data Files (for model training)\n",
    "TRAIN_ENGINEERED_PATH = 'train_fe.parquet'\n",
    "TEST_ENGINEERED_PATH = 'test_fe.parquet'\n",
    "PRODUCTS_ENGINEERED_PATH = 'products_fe.parquet'\n",
    "USERS_ENGINEERED_PATH = 'users_fe.parquet'\n",
    "\n",
    "# Model path\n",
    "MODEL_PATH = \"/kaggle/input/hack-inditex-task3-trainranker/lgbmranker_FULL_NEGNO_seedNO_.txt\"\n",
    "\n",
    "# OUTPUT\n",
    "# Similarity files\n",
    "TOP_SIM_PRODUCTS = '/kaggle/input/hack-inditex-task3-products/prod_sim_top_df.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a588c54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T21:20:42.968383Z",
     "iopub.status.busy": "2025-01-25T21:20:42.967924Z",
     "iopub.status.idle": "2025-01-25T21:20:42.977231Z",
     "shell.execute_reply": "2025-01-25T21:20:42.975970Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.016975,
     "end_time": "2025-01-25T21:20:42.979393",
     "exception": false,
     "start_time": "2025-01-25T21:20:42.962418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PolarsLoader:\n",
    "    def __init__(self, sampling=False, n_sample = 1_000_000, file_type='csv'):\n",
    "        \"\"\"\n",
    "        Initializes the PolarsLoader class.\n",
    "\n",
    "        Parameters:\n",
    "            sampling (bool): If True, loads a sample of 1,000,000 rows from the dataset. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.sampling = sampling\n",
    "        self.file_type = file_type\n",
    "        self.n_sample = n_sample\n",
    "    \n",
    "    def load_data(self, path, select_cols=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Loads the data from a CSV file.\n",
    "\n",
    "        Parameters:\n",
    "            path (str): Path to the CSV file.\n",
    "\n",
    "        Returns:\n",
    "            polars.DataFrame: Loaded DataFrame without selected columns.\n",
    "        \"\"\"\n",
    "        if self.sampling:\n",
    "            N_ROWS = self.n_sample\n",
    "        else:\n",
    "            N_ROWS = None\n",
    "\n",
    "        if self.file_type == 'csv':\n",
    "            # Read dataset as polars DataFrame\n",
    "            df = pl.read_csv(path, low_memory=True,\n",
    "                            batch_size=100_000, \n",
    "                            n_rows=N_ROWS,\n",
    "                            try_parse_dates=True,\n",
    "                            columns=select_cols,\n",
    "                            )\n",
    "        elif self.file_type == 'parquet':\n",
    "            # Read dataset as polars DataFrame\n",
    "            df = pl.read_parquet(path, low_memory=True,\n",
    "                            n_rows=N_ROWS,\n",
    "                            columns=select_cols,\n",
    "                            )\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Supported types are 'csv' and 'parquet'.\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def load_data_lazy(self, path):\n",
    "        \"\"\"\n",
    "        Loads the data from a CSV file lazily.\n",
    "\n",
    "        Parameters:\n",
    "            path (str): Path to the CSV file.\n",
    "\n",
    "        Returns:\n",
    "            polars.LazyFrame: LazyFrame object for the data.\n",
    "        \"\"\"\n",
    "        if self.sampling:\n",
    "            N_ROWS = self.n_sample\n",
    "        else:\n",
    "            N_ROWS = None\n",
    "\n",
    "        if self.file_type == 'csv':\n",
    "            # Read dataset as polars LazyFrame\n",
    "            lf = pl.scan_csv(path, low_memory=True,\n",
    "                                n_rows=N_ROWS,\n",
    "                                try_parse_dates=True,\n",
    "                                )\n",
    "        elif self.file_type == 'parquet':\n",
    "            # Read dataset as polars LazyFrame\n",
    "            lf = pl.scan_parquet(path, low_memory=True,\n",
    "                                n_rows=N_ROWS,\n",
    "                                )\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Supported types are 'csv' and 'parquet'.\")\n",
    "\n",
    "        return lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af25608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T21:20:42.995492Z",
     "iopub.status.busy": "2025-01-25T21:20:42.994950Z",
     "iopub.status.idle": "2025-01-25T21:20:43.007849Z",
     "shell.execute_reply": "2025-01-25T21:20:43.006698Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.023267,
     "end_time": "2025-01-25T21:20:43.011767",
     "exception": false,
     "start_time": "2025-01-25T21:20:42.988500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PRODUCTS:\n",
    "    def __init__(self, processed_path, engineered_path):\n",
    "        self.processed_path = processed_path\n",
    "        self.engineered_path = engineered_path\n",
    "\n",
    "    def load_data(self):\n",
    "        df = pl.read_parquet(self.processed_path, low_memory=True)\n",
    "        return df\n",
    "        \n",
    "    def normalize_embedding(self, products_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \n",
    "        # Normalized embedding\n",
    "        embeddings = np.stack(products_df['embedding'].to_numpy())\n",
    "        normalized = normalize(embeddings)\n",
    "        products_df = products_df.with_columns([pl.Series('embedding', normalized)])\n",
    "        \n",
    "        return products_df\n",
    "\n",
    "    def create_initial_extra_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \n",
    "        df = self.normalize_embedding(df)\n",
    "        \n",
    "        df = df.with_columns([\n",
    "            # Categorical frequencies\n",
    "            pl.col('color_id').count().over('family').alias('color_family_frequency'),\n",
    "            # Proporciones\n",
    "            (pl.col('discount').mean().over('family')).alias('family_discount_rate').cast(pl.Float32),\n",
    "            (pl.col('discount').mean().over('cod_section')).alias('section_discount_rate').cast(pl.Float32),\n",
    "            # Número de colores únicos en la familia\n",
    "            pl.col('color_id').n_unique().over('family').alias('family_unique_colors')\n",
    "        ])\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def run(self):\n",
    "        df = self.load_data()\n",
    "        df = self.create_initial_extra_features(df)\n",
    "        df.write_parquet(self.engineered_path)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f50cf7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T21:20:43.032184Z",
     "iopub.status.busy": "2025-01-25T21:20:43.031641Z",
     "iopub.status.idle": "2025-01-25T21:20:43.065536Z",
     "shell.execute_reply": "2025-01-25T21:20:43.063183Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.048824,
     "end_time": "2025-01-25T21:20:43.069069",
     "exception": false,
     "start_time": "2025-01-25T21:20:43.020245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class USERS:\n",
    "    def __init__(self, processed_path, engineered_path):\n",
    "        self.processed_path = processed_path\n",
    "        self.engineered_path = engineered_path\n",
    "\n",
    "    def load_and_process_users(self):\n",
    "        # Load and process user data\n",
    "        users = (\n",
    "            pl.read_parquet(self.processed_path, low_memory=True)\n",
    "            .rename({'country': 'user_country'})\n",
    "        )\n",
    "\n",
    "        # Select one entry per user based on HIGH F and HIGH R\n",
    "        users = (\n",
    "            users.sort(['user_id', 'F', 'R'], descending=[False, True, True])\n",
    "            .group_by('user_id')\n",
    "            .agg(pl.all().first())\n",
    "        )\n",
    "        return users\n",
    "\n",
    "    @staticmethod\n",
    "    def create_initial_extra_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "\n",
    "        df = df.with_columns([\n",
    "            # Average value per purchase\n",
    "            (pl.col('M') / pl.col('F')).alias('avg_value_per_purchase'),\n",
    "            # Purchase frequency rate (F normalized by time window)\n",
    "            (pl.col('F') / pl.col('R')).alias('purchase_rate').cast(pl.Float32),\n",
    "            # Value density (M normalized by time window)\n",
    "            (pl.col('M') / pl.col('R')).alias('spend_rate_per_day'),\n",
    "        ])\n",
    "\n",
    "        df = df.with_columns([\n",
    "            # Value-frequency relationship\n",
    "            (pl.col('M') * pl.col('F')).alias('total_value_frequency').cast(pl.Float32),\n",
    "            \n",
    "            # Recency-frequency relationship\n",
    "            ((pl.col('R') / pl.col('F'))).alias('avg_days_between_purchases').cast(pl.Float32),\n",
    "        ])\n",
    "\n",
    "        # Country stats and users relatives to country\n",
    "        contry_stats = df.group_by('user_country').agg([\n",
    "                pl.col('M').mean().alias('country_avg_monetary'),\n",
    "                pl.col('F').mean().alias('country_avg_frequency').cast(pl.Float32),\n",
    "                pl.col('R').mean().alias('country_avg_recency').cast(pl.Float32)\n",
    "            ])\n",
    "        df = df.join(contry_stats, on='user_country').with_columns([\n",
    "                (pl.col('M') / pl.col('country_avg_monetary')).alias('relative_monetary_value'),\n",
    "                (pl.col('F') / pl.col('country_avg_frequency')).alias('relative_frequency').cast(pl.Float32),\n",
    "                (pl.col('R') / pl.col('country_avg_recency')).alias('relative_recency').cast(pl.Float32),\n",
    "            ])\n",
    "\n",
    "        df = df.with_columns([\n",
    "            # High expend customer flag\n",
    "            (pl.col('M') > pl.col('M').mean().over('user_country')).cast(pl.Int8).alias('is_high_value_incountry'),\n",
    "            # Frequent buyer flag\n",
    "            (pl.col('F') > pl.col('F').mean().over('user_country')).cast(pl.Int8).alias('is_frequent_buyer_incountry'),\n",
    "            # Recent customer flag\n",
    "            (pl.col('R') > pl.col('R').mean().over('user_country')).cast(pl.Int8).alias('is_recent_customer_incountry'),\n",
    "            # High expend customer flag\n",
    "            (pl.col('M') > pl.col('M').mean()).cast(pl.Int8).alias('is_high_value'),\n",
    "            # Frequent buyer flag\n",
    "            (pl.col('F') > pl.col('F').mean()).cast(pl.Int8).alias('is_frequent_buyer'),\n",
    "            # Recent customer flag\n",
    "            (pl.col('R') > pl.col('R').mean()).cast(pl.Int8).alias('is_recent_customer'),\n",
    "        ])\n",
    "        \n",
    "        # Replace NaN and Inf values with 0\n",
    "        for col in ['purchase_rate', 'spend_rate_per_day', 'avg_days_between_purchases']:\n",
    "                if col in df.columns:\n",
    "                    df = df.with_columns(pl.when(pl.col(col).is_nan() | (pl.col(col)\n",
    "                                                                .is_infinite()))\n",
    "                                                                .then(0)\n",
    "                                                                .otherwise(pl.col(col))\n",
    "                                                                .alias(col),)\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def create_rfm_segments_and_ranks(users_df):\n",
    "        # Create quintiles (5 segments) for each metric\n",
    "        return users_df.with_columns([\n",
    "                    # Recency quintile (1 is most recent, 5 is least recent)\n",
    "                    pl.col('R')\n",
    "                        .rank(descending=True)  # High R is better\n",
    "                        .over('user_country')\n",
    "                        .map_batches(lambda x: pd.qcut(x, q=5, labels=False) + 1)\n",
    "                        .alias('r_segment').cast(pl.UInt8),\n",
    "                        \n",
    "                    # Frequency quintile (1 is highest frequency, 5 is lowest)\n",
    "                    pl.col('F')\n",
    "                        .rank(descending=True)   # higher F is better\n",
    "                        .over('user_country')\n",
    "                        .map_batches(lambda x: pd.qcut(x, q=5, labels=False) + 1)\n",
    "                        .alias('f_segment').cast(pl.UInt8),\n",
    "                        \n",
    "                    # Monetary quintile (1 is highest value, 5 is lowest)\n",
    "                    pl.col('M')\n",
    "                        .rank(descending=True)   # higher M is better\n",
    "                        .over('user_country')\n",
    "                        .map_batches(lambda x: pd.qcut(x, q=5, labels=False) + 1)\n",
    "                        .alias('m_segment').cast(pl.UInt8),\n",
    "\n",
    "                    # Individual percentile ranks\n",
    "                    pl.col('R').rank(descending=True)\n",
    "                        .over('user_country').alias('r_rank_in_country').cast(pl.Int32),\n",
    "                    pl.col('F').rank(descending=True)\n",
    "                        .over('user_country').alias('f_rank_in_country').cast(pl.Int32),\n",
    "                    pl.col('M').rank(descending=True)\n",
    "                        .over('user_country').alias('m_rank_in_country').cast(pl.Int32),\n",
    "                ])\n",
    "\n",
    "    def run(self):\n",
    "        df = self.load_and_process_users()\n",
    "        df = self.create_initial_extra_features(df)\n",
    "        df = self.create_rfm_segments_and_ranks(df)\n",
    "        df.write_parquet(self.engineered_path)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a8041b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T21:20:43.088482Z",
     "iopub.status.busy": "2025-01-25T21:20:43.088031Z",
     "iopub.status.idle": "2025-01-25T21:20:43.142712Z",
     "shell.execute_reply": "2025-01-25T21:20:43.141569Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.066563,
     "end_time": "2025-01-25T21:20:43.145049",
     "exception": false,
     "start_time": "2025-01-25T21:20:43.078486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TRAIN_TEST:\n",
    "    \n",
    "    def __init__(self, sampling=False, train_path=None, test_path=None, \n",
    "                        save_train_path=None, save_test_path=None, \n",
    "                        topN_global=10,topN_countries=10, topN_known_users=10,\n",
    "                        products_df=None, users_df=None,\n",
    "                        prod_sim_top_df_path=None, filter_similar=False, filter_similarN=5):\n",
    "\n",
    "        self.sampling = sampling\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.save_train_path = save_train_path\n",
    "        self.save_test_path = save_test_path\n",
    "        self.topN_global = topN_global\n",
    "        self.topN_countries = topN_countries\n",
    "        self.topN_known_users = topN_known_users\n",
    "        self.products_df = products_df\n",
    "        self.users_df = users_df\n",
    "        self.prod_sim_top_df_path = prod_sim_top_df_path\n",
    "        self.filter_similar = filter_similar\n",
    "        self.filter_similarN = filter_similarN\n",
    "        \n",
    "    def load_data(self):\n",
    "        loader = PolarsLoader(sampling=self.sampling, file_type='parquet')\n",
    "        train = loader.load_data(path=self.train_path)\n",
    "        test = loader.load_data(path=self.test_path)\n",
    "        return train, test\n",
    "    \n",
    "    def load_data_lazy(self):\n",
    "        loader = PolarsLoader(sampling=self.sampling, file_type='parquet')\n",
    "        train = loader.load_data_lazy(path=self.train_path)\n",
    "        test = loader.load_data_lazy(path=self.test_path)\n",
    "        return train, test\n",
    "    \n",
    "    def impute_train_test(self, df):\n",
    "        \n",
    "        # Cleaning and sorting\n",
    "        df = (df.drop(\"date\")\n",
    "            .sort(\"timestamp_local\")\n",
    "        )\n",
    "        \n",
    "        # Add tag for known user first\n",
    "        df = df.with_columns([\n",
    "            pl.when(pl.col(\"user_id\").is_null()).then(pl.lit(-1)).otherwise(pl.lit(1)).alias(\"known_user\").cast(pl.Int8),\n",
    "        ])\n",
    "\n",
    "        return df.with_columns([pl.col(\"user_id\").fill_null(-1).cast(pl.Int32),\n",
    "                        pl.col(\"pagetype\").fill_null(pl.col(\"pagetype\").mode()),\n",
    "                        ])\n",
    "    \n",
    "    def create_concatenated_features(self, train: pl.DataFrame, \n",
    "                               test: pl.DataFrame) -> tuple[pl.DataFrame, \n",
    "                                                            pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Create cart ratio features by concatenating train and test data, computing ratios,\n",
    "        then returning the separated dataframes with new features.\n",
    "        \n",
    "        Args:\n",
    "            train (pl.DataFrame): Training dataframe containing 'add_to_cart' column\n",
    "            test (pl.DataFrame): Test dataframe without 'add_to_cart' column\n",
    "        \n",
    "        Returns:\n",
    "            tuple[pl.DataFrame, pl.DataFrame]: Tuple containing (train, test) dataframes with new features\n",
    "        \"\"\"\n",
    "        # Create dummy test dataframe with placeholder add_to_cart column\n",
    "        dummy_test = (test\n",
    "                    .with_columns([pl.lit(None).alias('add_to_cart')])\n",
    "                    .select(sorted(train.columns, reverse=True))\n",
    "                    .with_columns([pl.lit(0).alias('flag')])\n",
    "                      \n",
    "        )\n",
    "        \n",
    "        # Prepare train dataframe with flag\n",
    "        dummy_train = (train\n",
    "                    .select(sorted(train.columns, reverse=True))\n",
    "                    .with_columns([pl.lit(1).alias('flag')])\n",
    "        )\n",
    "        \n",
    "        # Concatenate train and test\n",
    "        traintest_concat = pl.concat([dummy_train, dummy_test])\n",
    "        # Calculate page type cart addition ratio\n",
    "        page_cart_ratio = (traintest_concat.group_by(\"pagetype\")\n",
    "                        .agg(pl.col(\"add_to_cart\").mean().alias(\"page_cart_ratio\").cast(pl.Float32))\n",
    "                        .fill_null(pl.lit(-1))\n",
    "                        .with_columns(pl.col(\"pagetype\").cast(pl.UInt8))\n",
    "        )\n",
    "        traintest_concat = traintest_concat.join(page_cart_ratio, on=\"pagetype\", how=\"left\")\n",
    "        # Calculate device type cart addition ratio\n",
    "        device_cart_ratio = (traintest_concat.group_by(\"device_type\")\n",
    "            .agg(pl.col(\"add_to_cart\").mean().alias(\"device_cart_ratio\").cast(pl.Float32))\n",
    "        )\n",
    "        traintest_concat = traintest_concat.join(device_cart_ratio, on=\"device_type\", how=\"left\")\n",
    "        # Calculate country cart addition ratio\n",
    "        country_cart_ratio = (traintest_concat.group_by(\"country\")\n",
    "            .agg(pl.col(\"add_to_cart\").mean().alias(\"country_cart_ratio\").cast(pl.Float32))\n",
    "        )\n",
    "        traintest_concat = traintest_concat.join(country_cart_ratio, on=\"country\", how=\"left\")\n",
    "        \n",
    "        # Calculate cumulative user features\n",
    "        traintest_concat = traintest_concat.with_columns([\n",
    "                pl.col(\"add_to_cart\").cum_sum().over(\"user_id\").alias(\"user_previous_cart_additions\"),\n",
    "                pl.col(\"user_id\").cum_count().over(\"user_id\").alias(\"user_previous_interactions\")\n",
    "            ])\n",
    "        \n",
    "        # Split back into train and test based on flag\n",
    "        train_processed = traintest_concat.filter(pl.col(\"flag\") == 1).drop(\"flag\")\n",
    "        test_processed = traintest_concat.filter(pl.col(\"flag\") == 0).drop([\"flag\", \"add_to_cart\"])\n",
    "        \n",
    "        return train_processed, test_processed\n",
    "\n",
    "    def feature_engineering_no_candidate_dependent(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "\n",
    "        # Cleaning and sorting\n",
    "        #df_ = (df.drop(\"date\")\n",
    "        #    .sort(\"timestamp_local\")\n",
    "        #)\n",
    "\n",
    "        df_ = df.with_columns([\n",
    "            # Calculate the difference in timestamps within each session\n",
    "            (pl.col(\"timestamp_local\").diff().over(\"session_id\").cast(pl.Float32) / 1_000_000)\n",
    "            .round(1).alias(\"seconds_since_last_interaction\"),\n",
    "            # Total session duration in seconds\n",
    "            ((pl.col(\"timestamp_local\").max() - pl.col(\"timestamp_local\").min()).over(\"session_id\").cast(pl.Float32) / 1_000_000)\n",
    "            .round(1).alias(\"total_session_time\"),\n",
    "        ]).fill_null(strategy=\"zero\")\n",
    "\n",
    "        df_ = df_.with_columns([\n",
    "                    pl.col(\"seconds_since_last_interaction\").shift(-1).over(\"session_id\").alias(\"interaction_length\"),\n",
    "        ]).fill_null(strategy=\"zero\")\n",
    "\n",
    "        # Date features\n",
    "        df_ = df_.with_columns([\n",
    "            # Extracting day number\n",
    "            pl.col(\"timestamp_local\").dt.day().alias(\"day_number\"),\n",
    "            \n",
    "            # Extracting weekday number\n",
    "            pl.col(\"timestamp_local\").dt.weekday().alias(\"weekday_number\"),\n",
    "            \n",
    "            # Extracting weekday name\n",
    "            # pl.col(\"timestamp_local\").dt.strftime(\"%A\").alias(\"weekday_name\").cast(pl.Categorical),\n",
    "            \n",
    "            # Extracting hour\n",
    "            pl.col(\"timestamp_local\").dt.hour().alias(\"hour\")\n",
    "        ])\n",
    "        \n",
    "        df_ = df_.with_columns([\n",
    "                        pl.when((pl.col(\"hour\") >= 6) & (pl.col(\"hour\") < 12)).then(pl.lit(0))\n",
    "                        .when((pl.col(\"hour\") >= 12) & (pl.col(\"hour\") < 18)).then(pl.lit(1))\n",
    "                        .when((pl.col(\"hour\") >= 18) & (pl.col(\"hour\") < 24)).then(pl.lit(2))\n",
    "                        .otherwise(pl.lit(3))\n",
    "                        .cast(pl.UInt8)\n",
    "                        .alias(\"day_frame\")\n",
    "                    ])\n",
    "        # Others\n",
    "        df_ = df_.with_columns(pl.col('timestamp_local').count().over('session_id').alias('total_session_interactions'))\n",
    "        \n",
    "        return df_\n",
    "    \n",
    "    def select_global_candidates(self, train_df):\n",
    "\n",
    "        if self.topN_global:\n",
    "            cart_counts = (train_df\n",
    "                .group_by('partnumber')\n",
    "                .agg(pl.col('add_to_cart').count())\n",
    "                .sort('add_to_cart', descending=True)\n",
    "                )\n",
    "            \n",
    "            top_global_prods = cart_counts['partnumber'].head(self.topN_global).to_list()\n",
    "    \n",
    "            return top_global_prods\n",
    "        return []\n",
    "\n",
    "    def select_country_candidates(self, train_df):\n",
    "        country_top_products = (train_df\n",
    "                              .group_by([\"country\", \"partnumber\"])  # Group by country and product\n",
    "                              .agg([\n",
    "                                  pl.col(\"add_to_cart\").sum().alias(\"total_add_to_cart\")  # Sum of add_to_cart for popularity\n",
    "                              ])\n",
    "                              .sort(by=[\"country\", \"total_add_to_cart\"], descending=True)  # Sort by country and popularity\n",
    "                              .group_by(\"country\")\n",
    "                              .agg([\n",
    "                                  pl.col(\"partnumber\").head(self.topN_countries).alias(\"top_partnumbers\")  # Top N products per country\n",
    "                              ])\n",
    "                         )\n",
    "\n",
    "        exploded_country_products = (country_top_products.explode(\"top_partnumbers\").rename({\"top_partnumbers\": \"partnumber\"})\n",
    "                .with_columns(pl.lit(1).alias(\"synthetic_candidate\").cast(pl.Int64)))\n",
    "\n",
    "        return exploded_country_products\n",
    "\n",
    "    def select_kwown_user_candidates(self, train_df):\n",
    "\n",
    "        kwown_users_top = (train_df\n",
    "                            .filter((pl.col(\"user_id\") != -1) & (pl.col(\"add_to_cart\") == 0))\n",
    "                            .select(['user_id', 'partnumber', 'product_interaction_count', 'interaction_length'])\n",
    "                            .sort(by=['product_interaction_count', 'interaction_length'], descending=[True, True])\n",
    "                            .group_by(\"user_id\")\n",
    "                                  .agg([\n",
    "                                      pl.col(\"partnumber\").head(self.topN_known_users).alias(\"top_partnumbers\")  # Top N interacted products for known users\n",
    "                                  ])\n",
    "        )\n",
    "        \n",
    "        kwown_user_candidates = kwown_users_top.explode(\"top_partnumbers\").rename({\"top_partnumbers\": \"partnumber\"}).with_columns(pl.lit(1).alias(\"synthetic_candidate\").cast(pl.Int64))\n",
    "\n",
    "        return kwown_user_candidates\n",
    "\n",
    "    def select_topsimilar_candidates(self):\n",
    "\n",
    "        top10_sim_df = pl.read_parquet(self.prod_sim_top_df_path)\n",
    "    \n",
    "        if self.filter_similar:\n",
    "            top10_sim_df = top10_sim_df.with_columns(pl.col('top_10_cos_partnumber').list.head(self.filter_similarN))\n",
    "    \n",
    "        top10_sim_df = (top10_sim_df.explode(\"top_10_cos_partnumber\")\n",
    "                    .with_columns(pl.col('partnumber').cast(pl.UInt16))\n",
    "                    .filter(~(pl.col('partnumber')==pl.col('top_10_cos_partnumber'))) #There are some similar to itself\n",
    "                   )\n",
    "        \n",
    "        return top10_sim_df.with_columns(pl.lit(1).alias(\"synthetic_candidate\").cast(pl.Int64))\n",
    "\n",
    "    def add_candidates_to_test(self, test_df: pl.DataFrame, global_candidate_products: list,\n",
    "                           exploded_country_products: pl.DataFrame,\n",
    "                          kwown_user_candidates: pl.DataFrame,\n",
    "                          top10_sim_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Adds candidate products as new rows for each session in test data\"\"\"   \n",
    "        \n",
    "        ###################\n",
    "        ## GLOBAL ########\n",
    "        print(f\"Adding {len(global_candidate_products)} global propular products to test data...\")\n",
    "        if self.topN_global:            \n",
    "            test_session_ids = test_df[\"session_id\"].unique()\n",
    "    \n",
    "            # Create a dataframe with all the candidate products for each session\n",
    "            candidates = [\n",
    "                {\"session_id\": sid, \"partnumber\": pn, \"synthetic_candidate\": 1}\n",
    "                for sid in test_session_ids \n",
    "                for pn in global_candidate_products\n",
    "            ]\n",
    "            candidates_df = pl.DataFrame(candidates).with_columns([\n",
    "                pl.col(\"session_id\").cast(pl.UInt32),\n",
    "                pl.col(\"partnumber\").cast(pl.UInt16)\n",
    "            ])\n",
    "            \n",
    "            # Add the candidate products to the test data\n",
    "            test_extended = pl.concat([test_df, candidates_df], how=\"diagonal\").unique()\n",
    "\n",
    "        else:\n",
    "            test_extended = test_df.clone()\n",
    "        #####################\n",
    "        ## COUNTRIES ########\n",
    "        print(f\"Adding {self.topN_countries} country popular products to test data...\")\n",
    "        # Extract unique session_id and country combinations from the test data\n",
    "        session_country_mapping = test_df.select([\"session_id\", \"country\"]).unique()\n",
    "    \n",
    "        # Perform a cross-join to generate all combinations of session_id and country products\n",
    "        country_session_combinations = (\n",
    "            session_country_mapping.join(exploded_country_products, on=\"country\", how=\"inner\")\n",
    "        )\n",
    "\n",
    "        test_extended = pl.concat([test_extended, country_session_combinations], how=\"diagonal\").unique()\n",
    "\n",
    "        #######################\n",
    "        ## KNOWN USERS ########\n",
    "        print(f\"Adding {self.topN_known_users} candidate products for known users to test data...\")\n",
    "        test_users = test_df.select([\"session_id\", \"user_id\"]).unique()\n",
    "        test_user_candidates_combination = test_users.join(kwown_user_candidates, on=\"user_id\", how=\"inner\")\n",
    "        test_extended = pl.concat([test_extended, test_user_candidates_combination], how=\"diagonal\").unique()\n",
    "\n",
    "        ###############################\n",
    "        ## TOP SIMILAR TO TEST ########\n",
    "        print(f\"Adding {10 if not self.filter_similar else self.filter_similarN} candidate products to test data...\")\n",
    "        test_in_products = test_df.select([\"session_id\", \"partnumber\"]).unique()\n",
    "    \n",
    "        sessions_partnumber_combinations = (test_in_products.join(top10_sim_df, on=\"partnumber\", how=\"inner\")\n",
    "                                            .with_columns(pl.concat_str(\n",
    "                                                    [pl.col('session_id'),\n",
    "                                                     pl.col('partnumber')],\n",
    "                                                    ).alias(\"tmp_key\").cast(pl.Int64))\n",
    "                                   ).drop('partnumber').rename({'top_10_cos_partnumber':'partnumber'})\n",
    "        #Create temporal merging key\n",
    "        test_extended = (test_extended.with_columns(pl.concat_str(\n",
    "                        [pl.col('session_id'),\n",
    "                         pl.col('partnumber')],\n",
    "                        ).alias(\"tmp_key\").cast(pl.Int64)))\n",
    "\n",
    "        test_extended = pl.concat([test_extended, sessions_partnumber_combinations], how=\"diagonal\").unique().drop('tmp_key')\n",
    "         \n",
    "        return test_extended.unique() #Drops candidates added more than once\n",
    "\n",
    "    def fill_test_nulls_candidates(self, test_extended: pl.DataFrame) -> pl.DataFrame:\n",
    "    \n",
    "        #Fill missing values with generated: Faster version\n",
    "        backfilled_cols = ['timestamp_local',\n",
    "                            'user_id',\n",
    "                            'country',\n",
    "                            'device_type',\n",
    "                            'pagetype',\n",
    "                            'total_session_time',\n",
    "                            'day_number',\n",
    "                            'weekday_number',\n",
    "                            # 'weekday_name',\n",
    "                            'hour',\n",
    "                            'day_frame']\n",
    "        \n",
    "        statsfilled_cols = ['seconds_since_last_interaction',\n",
    "                            'interaction_length']\n",
    "        \n",
    "        test_extended = (test_extended\n",
    "                        .sort([\"session_id\", \"timestamp_local\"], descending=[False, True])\n",
    "                        .with_columns([\n",
    "                            pl.col(col).fill_null(strategy='backward').over(\"session_id\") \n",
    "                            for col in backfilled_cols\n",
    "                        ])\n",
    "                        .with_columns([\n",
    "                            pl.col(col).fill_null(strategy='mean').over(\"session_id\") \n",
    "                            for col in statsfilled_cols\n",
    "                        ])\n",
    "                    )\n",
    "        return test_extended\n",
    "    \n",
    "    def feature_engineering_candidate_dependent(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \n",
    "        df = df.with_columns([\n",
    "            # Assign a cumulative count for each partnumber within a session\n",
    "            pl.col(\"partnumber\").cum_count().over([\"session_id\", \"partnumber\"]).alias(\"product_interaction_count\")\n",
    "        ])\n",
    "\n",
    "        # Fill final nulls and correct issues\n",
    "        remain_nulls = [\n",
    "            'known_user',\n",
    "            'page_cart_ratio',\n",
    "            'device_cart_ratio',\n",
    "            'country_cart_ratio',\n",
    "            'user_previous_cart_additions',\n",
    "            'user_previous_interactions',\n",
    "            'total_session_interactions'\n",
    "        ]\n",
    "        \n",
    "        df = df.with_columns(\n",
    "            pl.col(col).fill_null(strategy='max').over(\"session_id\") \n",
    "                                    for col in remain_nulls\n",
    "        )\n",
    "\n",
    "        df = df.with_columns([\n",
    "            pl.when(pl.col(\"known_user\") == -1)\n",
    "                .then(pl.lit(-1))\n",
    "                .otherwise(pl.col(\"user_previous_cart_additions\"))\n",
    "                .alias(\"user_previous_cart_additions\").cast(pl.Int16),\n",
    "            pl.when(pl.col(\"known_user\") == -1)\n",
    "                .then(pl.lit(-1))\n",
    "                .otherwise(pl.col(\"user_previous_interactions\"))\n",
    "                .alias(\"user_previous_interactions\").cast(pl.Int16),    \n",
    "        ])\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def merge_datasets(self, df, products_df, users_df):\n",
    "        df = (df\n",
    "              .drop('timestamp_local')\n",
    "              .join(products_df.drop('embedding'), on='partnumber', how=\"left\"))\n",
    "        df = df.join(users_df, on='user_id', how=\"left\")\n",
    "        return df\n",
    "    \n",
    "    def run(self, process_train=True):\n",
    "        \"\"\"\n",
    "        Processes train and test data, with options to skip training data processing\n",
    "        and dynamically update candidates for the test dataset.\n",
    "        \n",
    "        Args:\n",
    "            process_train (bool): Whether to process the training data.\n",
    "                \n",
    "        Returns:\n",
    "            train_eng (pl.DataFrame): Processed training data (if process_train is True).\n",
    "            test_extended (pl.DataFrame): Processed and candidate-augmented test data.\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        print(\"Loading train and test data...\")\n",
    "        train, test = self.load_data()\n",
    "        # train, test = self.load_data_lazy()\n",
    "        print(\"Imputing missing values...\")\n",
    "        train = self.impute_train_test(train)\n",
    "        test = self.impute_train_test(test)\n",
    "        print(\"Creating concatenated features...\")\n",
    "        train, test = self.create_concatenated_features(train, test)\n",
    "\n",
    "        if process_train:\n",
    "            print(\"Processing train data...\")\n",
    "            train_eng = self.feature_engineering_no_candidate_dependent(train)\n",
    "            train_eng = self.feature_engineering_candidate_dependent(train_eng)\n",
    "\n",
    "            #######################\n",
    "            ### IF LAZY LOADING ###\n",
    "            #######################\n",
    "            # train_eng = train_eng.collect()\n",
    "        else:\n",
    "            print(\"Skipping train processing. Loading preprocessed data...\")\n",
    "            train_eng = pl.read_parquet(self.save_train_path)\n",
    "\n",
    "        # Test processing\n",
    "        print(\"Processing test data...\")\n",
    "        test_eng = self.feature_engineering_no_candidate_dependent(test)\n",
    "\n",
    "        # Candidate selection\n",
    "        print(\"Selecting candidates...\")\n",
    "        global_candidates = self.select_global_candidates(train_eng)\n",
    "        exploded_country_products = self.select_country_candidates(train_eng)\n",
    "        kwown_user_candidates = self.select_kwown_user_candidates(train_eng)\n",
    "        top10_sim_df = self.select_topsimilar_candidates()\n",
    "\n",
    "        # Add candidates to test\n",
    "        #######################\n",
    "        ### IF LAZY LOADING ###\n",
    "        #######################\n",
    "        # test_eng = test_eng.collect()\n",
    "\n",
    "        test_extended = self.add_candidates_to_test(test_df = test_eng,\n",
    "                                                    global_candidate_products = global_candidates,\n",
    "                                                    exploded_country_products = exploded_country_products,\n",
    "                                                    kwown_user_candidates = kwown_user_candidates,\n",
    "                                                    top10_sim_df = top10_sim_df)\n",
    "        print(\"Test set extended.\")\n",
    "        test_extended = self.fill_test_nulls_candidates(test_extended)\n",
    "        test_extended = self.feature_engineering_candidate_dependent(test_extended)\n",
    "        print(\"Filling candidate nulls and generating last features...\")\n",
    "        \n",
    "        # Merge with users and products\n",
    "        print(\"Merging with users and products data...\")\n",
    "        final_train = self.merge_datasets(train_eng, self.products_df, self.users_df)\n",
    "        final_test = self.merge_datasets(test_extended, self.products_df, self.users_df)\n",
    "\n",
    "        print(\"Saving final data to parquet...\")\n",
    "        print(\"Final size of test data:\", len(final_test))\n",
    "        final_test.write_parquet(self.save_test_path)\n",
    "        if process_train:\n",
    "            print(\"Saving parquet file...\")\n",
    "            final_train.write_parquet(self.save_train_path)\n",
    "            \n",
    "        print(\"Data processing completed.\")\n",
    "        \n",
    "        return final_train, final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32bd9c42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T21:20:43.159740Z",
     "iopub.status.busy": "2025-01-25T21:20:43.159363Z",
     "iopub.status.idle": "2025-01-25T21:24:47.663126Z",
     "shell.execute_reply": "2025-01-25T21:24:47.661328Z"
    },
    "papermill": {
     "duration": 244.517616,
     "end_time": "2025-01-25T21:24:47.668696",
     "exception": false,
     "start_time": "2025-01-25T21:20:43.151080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product data...\n",
      "Processing user data...\n",
      "Loading train and test data...\n",
      "Imputing missing values...\n",
      "Creating concatenated features...\n",
      "Processing train data...\n",
      "Processing test data...\n",
      "Selecting candidates...\n",
      "Adding 0 global propular products to test data...\n",
      "Adding 5 country popular products to test data...\n",
      "Adding 5 candidate products for known users to test data...\n",
      "Adding 5 candidate products to test data...\n",
      "Test set extended.\n",
      "Filling candidate nulls and generating last features...\n",
      "Merging with users and products data...\n",
      "Saving final data to parquet...\n",
      "Final size of test data: 187613\n",
      "Saving parquet file...\n",
      "Data processing completed.\n"
     ]
    }
   ],
   "source": [
    "# Configurable options\n",
    "PROCESS_PRODS = True\n",
    "PROCESS_TRAIN = True \n",
    "SAMPLE_TRAIN = False ################## SAMPLING\n",
    "PROCESS_USERS = True\n",
    "\n",
    "TOP_N_GLOBAL = None  # Number of global most popular candidates\n",
    "TOP_N_COUNTRIES = 5\n",
    "TOP_N_KNOWN_USERS = 5\n",
    "REDUCE_SIMILAR_N = True\n",
    "TOP_N_SIMILAR = 5\n",
    "\n",
    "def main():\n",
    "    if PROCESS_PRODS:\n",
    "        print(\"Processing product data...\")\n",
    "        products = PRODUCTS(PRODUCTS_PARQUET_PATH_IMPUTED, \n",
    "                            PRODUCTS_ENGINEERED_PATH)\n",
    "        products_df = products.run()\n",
    "\n",
    "    if PROCESS_USERS:\n",
    "        print(\"Processing user data...\")\n",
    "        users = USERS(USERS_DATA_PATH, USERS_ENGINEERED_PATH)\n",
    "        users_df = users.run()\n",
    "\n",
    "    with StringCache():\n",
    "        # Initialize processor\n",
    "        traintest_processor = TRAIN_TEST(\n",
    "            sampling=SAMPLE_TRAIN, \n",
    "            train_path=TRAIN_PARQUET_PATH, \n",
    "            test_path=TEST_PARQUET_PATH,\n",
    "            save_train_path=TRAIN_ENGINEERED_PATH,\n",
    "            save_test_path=TEST_ENGINEERED_PATH,\n",
    "            topN_global=TOP_N_GLOBAL, topN_countries=TOP_N_COUNTRIES, topN_known_users=TOP_N_KNOWN_USERS,\n",
    "            users_df=users_df,\n",
    "            products_df=products_df,\n",
    "            prod_sim_top_df_path=TOP_SIM_PRODUCTS, filter_similar=REDUCE_SIMILAR_N, filter_similarN=TOP_N_SIMILAR\n",
    "        )\n",
    "\n",
    "        # Run processing\n",
    "        train, test = traintest_processor.run(\n",
    "            process_train=PROCESS_TRAIN, \n",
    "        )\n",
    "    return train, test\n",
    "    \n",
    "train, test = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445e6ac",
   "metadata": {
    "papermill": {
     "duration": 0.004925,
     "end_time": "2025-01-25T21:24:47.689501",
     "exception": false,
     "start_time": "2025-01-25T21:24:47.684576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Predict and export predictions¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3f634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T21:24:47.701720Z",
     "iopub.status.busy": "2025-01-25T21:24:47.701296Z",
     "iopub.status.idle": "2025-01-25T21:24:50.448447Z",
     "shell.execute_reply": "2025-01-25T21:24:50.447524Z"
    },
    "papermill": {
     "duration": 2.75603,
     "end_time": "2025-01-25T21:24:50.450625",
     "exception": false,
     "start_time": "2025-01-25T21:24:47.694595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "model = lgb.Booster(model_file=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18627f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T21:24:50.464289Z",
     "iopub.status.busy": "2025-01-25T21:24:50.463572Z",
     "iopub.status.idle": "2025-01-25T21:24:50.469586Z",
     "shell.execute_reply": "2025-01-25T21:24:50.468669Z"
    },
    "papermill": {
     "duration": 0.014252,
     "end_time": "2025-01-25T21:24:50.471429",
     "exception": false,
     "start_time": "2025-01-25T21:24:50.457177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    " #'partnumber',\n",
    " 'pagetype',\n",
    " 'known_user',\n",
    " 'device_type',\n",
    " 'country',\n",
    " 'page_cart_ratio',\n",
    " 'device_cart_ratio',\n",
    " 'country_cart_ratio',\n",
    " 'user_previous_cart_additions',\n",
    " 'user_previous_interactions',\n",
    " 'seconds_since_last_interaction',\n",
    " 'total_session_time',\n",
    " 'interaction_length',\n",
    " 'day_number',\n",
    " 'weekday_number',\n",
    " 'hour',\n",
    " 'day_frame',\n",
    " 'total_session_interactions',\n",
    " 'product_interaction_count',\n",
    " 'discount',\n",
    " 'color_id',\n",
    " 'cod_section',\n",
    " 'family',\n",
    " 'color_family_frequency',\n",
    " 'family_discount_rate',\n",
    " 'section_discount_rate',\n",
    " 'family_unique_colors',\n",
    " 'user_country',\n",
    " 'R',\n",
    " 'F',\n",
    " 'M',\n",
    " 'avg_value_per_purchase',\n",
    " 'purchase_rate',\n",
    " 'spend_rate_per_day',\n",
    " 'total_value_frequency',\n",
    " 'avg_days_between_purchases',\n",
    " 'country_avg_monetary',\n",
    " 'country_avg_frequency',\n",
    " 'country_avg_recency',\n",
    " 'relative_monetary_value',\n",
    " 'relative_frequency',\n",
    " 'relative_recency',\n",
    " 'is_high_value_incountry',\n",
    " 'is_frequent_buyer_incountry',\n",
    " 'is_recent_customer_incountry',\n",
    " 'is_high_value',\n",
    " 'is_frequent_buyer',\n",
    " 'is_recent_customer',\n",
    " 'r_segment',\n",
    " 'f_segment',\n",
    " 'm_segment',\n",
    " 'r_rank_in_country',\n",
    " 'f_rank_in_country',\n",
    " 'm_rank_in_country'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7e4038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T21:24:50.483805Z",
     "iopub.status.busy": "2025-01-25T21:24:50.483318Z",
     "iopub.status.idle": "2025-01-25T21:25:13.046941Z",
     "shell.execute_reply": "2025-01-25T21:25:13.045393Z"
    },
    "papermill": {
     "duration": 22.572718,
     "end_time": "2025-01-25T21:25:13.049655",
     "exception": false,
     "start_time": "2025-01-25T21:24:50.476937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    session_id  partnumber  predicted_score\n",
      "40         746       13295         2.952565\n",
      "11         746       26738         2.825188\n",
      "10         746       25688         2.762497\n",
      "14         746        9145         2.741379\n",
      "8          746        3061         2.735885\n"
     ]
    }
   ],
   "source": [
    "def predict(test_df, feature_cols, model):\n",
    "    \"\"\"\n",
    "    Hace predicciones en nuevos datos\n",
    "    \"\"\"\n",
    "    test_pandas = test.to_pandas() # Just in case to retain order\n",
    "    predictions = model.predict(test_pandas[feature_cols])\n",
    "    \n",
    "    # Devolver los top 5 productos por sesión\n",
    "    test_pandas['predicted_score'] = predictions\n",
    "    \n",
    "    recommendations = []\n",
    "    for session_id in test_pandas['session_id'].unique():\n",
    "        session_preds = test_pandas[test_pandas['session_id'] == session_id]\n",
    "        top_5 = session_preds.nlargest(5, 'predicted_score')[['session_id', 'partnumber', 'predicted_score']]\n",
    "        recommendations.append(top_5)\n",
    "        \n",
    "    return pd.concat(recommendations)\n",
    "\n",
    "recommendations = predict(test, feature_cols, model)\n",
    "print(recommendations.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c3f4fb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T21:25:13.071240Z",
     "iopub.status.busy": "2025-01-25T21:25:13.070785Z",
     "iopub.status.idle": "2025-01-25T21:25:13.287559Z",
     "shell.execute_reply": "2025-01-25T21:25:13.286240Z"
    },
    "papermill": {
     "duration": 0.226854,
     "end_time": "2025-01-25T21:25:13.289679",
     "exception": false,
     "start_time": "2025-01-25T21:25:13.062825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().strftime(\"ene%d-%H%Mhs\")\n",
    "file_tag = f\"{TOP_N_GLOBAL}-{TOP_N_COUNTRIES}-{TOP_N_KNOWN_USERS}_{REDUCE_SIMILAR_N}\"\n",
    "\n",
    "# Group by session_id and convert partnumbers to a list\n",
    "grouped = recommendations.groupby('session_id')['partnumber'].apply(list).to_dict()\n",
    "\n",
    "# Build final JSON dict (session IDs as strings are more standard in JSON)\n",
    "final_json = {\n",
    "    \"target\": {\n",
    "        str(sess_id): parts for sess_id, parts in grouped.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'predictions_3_{now}_{file_tag}.json', 'w') as f:\n",
    "    json.dump(final_json, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6438569,
     "sourceId": 10392372,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 218951718,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219238976,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 280.369348,
   "end_time": "2025-01-25T21:25:17.558005",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-25T21:20:37.188657",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
